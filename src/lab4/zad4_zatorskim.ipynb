{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Labolatorium 4\n",
    "## Wyszukiwarka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Przygotowanie zbioru (> 1000 elementów) dokumentów tekstowych w języku angielskim.\n",
    "\n",
    "Dokumenty znajdują się w folderze `documents`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. & 3. Określenie słów kluczowych (termów) potrzebnych do wyznaczenia *bag-of-words* dla każdego dokumentów oraz wyznaczenie *bag-of-words* dla każdego dokumentu $d_j$\n",
    "\n",
    "W naszym przypadku bazą termów będzie unia wszystkich słów występujących we wszystkich tekstach. Postaramy się pominąć wszelkiego rodzaju znaki interpunkcyjne oraz często występujące słowa w języku angielskim, które nie mają większego znaczenia (np. 'a', 'this', 'of')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przydatne importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zapisywanie i odczyt najważniejszych struktur z pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def save_counter_to_file(file_name, bag_of_words):\n",
    "    text = \"\"\n",
    "    for word, count in bag_of_words.items():\n",
    "        text = text + word + \" \" + str(count) + \"\\n\"\n",
    "    filepath = os.path.join(os.getcwd(), file_name)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as article_file:\n",
    "        article_file.write(text)\n",
    "\n",
    "        \n",
    "def load_counter_from_file(file_name):\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "    bag_of_words = Counter()\n",
    "\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as text:\n",
    "            for line in text:\n",
    "                elems = line.split()  # elems[0] - word, elems[1] - word count\n",
    "                bag_of_words[elems[0]] = int(elems[1])\n",
    "                \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja, która na podstawie tekstu tworzy wektor *bag-of-words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words_from_text(text):\n",
    "    # make all article lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # divide into words (it still includes punctuation)\n",
    "    words = [word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "\n",
    "    # remove meaningless English words such as 'the', 'a' etc.\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in english_stop_words]\n",
    "\n",
    "    # remove punctuation\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.add(\"...\")\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # stem words - this makes words with the same meaning equal, e.g. responsibility, responsible => respons\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # remove meaningless 1 or 2 chars words\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "\n",
    "    # create a bag_of_words based on this article\n",
    "    bag_of_words = Counter(words)\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funkcja, która dla każdego artykułu:\n",
    "- wczytuje jego tekst i na jego podstawie tworzy wektor cech *bag-of-words* ${d_j}$\n",
    "- Zapisuje ten wektor do odpowiedniego pliku w katalogu `bags`\n",
    "\n",
    "W końcu na podstawie wszystkich *bag-of-words* tworzy słownik bazowy dla wszystkich dokumentów i zapisuje go do pliku `base_dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def process_articles(source_directory = \"articles\"):\n",
    "    base_dictionary = Counter()\n",
    "    \n",
    "    curr_path = os.getcwd()\n",
    "    source_path = os.path.join(curr_path, source_directory)\n",
    "    bags_dir = os.path.join(curr_path, \"bags\")\n",
    "    if not os.path.exists(bags_dir):\n",
    "        os.makedirs(bags_dir)\n",
    "        \n",
    "    for file_name in os.listdir(source_path):\n",
    "        file_path = os.path.join(source_path, file_name)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as text:\n",
    "\n",
    "            bag_of_words = get_bag_of_words_from_text(text.read())\n",
    "            base_dictionary += bag_of_words\n",
    "            save_counter_to_file(f\"bags/{file_name}\", bag_of_words)\n",
    "    \n",
    "    save_counter_to_file(\"base_dictionary\", base_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Budowanie rzadkiej macierzy wektorów cech *term-by-document matrix*.\n",
    "Wektory ułożone są kolumnowo $A_{m \\times n} = [d_1|d_2|...|d_n]$ (m jest liczbą termów w słowniku, n jest liczbą dokumentów)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa funkcja tworzy macierz cech (pierwsza zwracana wartość), ponadtdo zwraca opis osi Y (termy w odpowiedniej kolejności) jako drugą wartość oraz opis osi X (nazwy dokumentów w odpowiedniej kolejności) jako trzecią wartość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_term_by_document_matrix(base_ditionary_name = \"base_dictionary\", bags_dir = \"bags\"):\n",
    "    base_dictionary = load_counter_from_file(base_ditionary_name)\n",
    "    terms_list = list(base_dictionary)\n",
    "    \n",
    "    \n",
    "    bag_names = os.listdir(bags_dir)\n",
    "    N = len(bag_names)\n",
    "    M = len(terms_list)\n",
    "    \n",
    "    term_by_document_matrix = sparse.lil_matrix((M, N))\n",
    "\n",
    "    for j, file_name in enumerate(bag_names):\n",
    "        bag_of_terms = load_counter_from_file(os.path.join(bags_dir, file_name))\n",
    "\n",
    "        \n",
    "        for i, term in enumerate(terms_list):\n",
    "            term_by_document_matrix[i, j] = bag_of_terms[term]\n",
    "            \n",
    "    \n",
    "    return term_by_document_matrix.tocsr(), terms_list, bag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Przetworzenie wstępnie otrzymanego zbioru danych przez *inverse document frequency*. \n",
    "\n",
    "$ IDF(w) = log(\\frac{N}{n_w}) $\n",
    "\n",
    "gdzie:\\\n",
    "$n_w$ - liczba dokumentów, w których występuje słowo $w$\\\n",
    "$N$ - całkowita liczba dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def IDF(row):\n",
    "    N = row.shape[1]\n",
    "    n_w = row.count_nonzero()\n",
    "    return np.log(N / n_w)\n",
    "\n",
    "def tf_to_idf(term_by_document_matrix):\n",
    "    for i in range(term_by_document_matrix.shape[0]):\n",
    "        idf = IDF(term_by_document_matrix[i,:])\n",
    "        term_by_document_matrix[i,:] *= idf\n",
    "    return term_by_document_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Czas przetestować wyżej napisane funkcj i utworzyć pełnoprawną macierz *term-by-document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.09861229, 0.        , 0.        ],\n",
       "        [1.09861229, 0.        , 0.        ],\n",
       "        [1.09861229, 0.        , 0.        ],\n",
       "        [1.09861229, 0.        , 0.        ],\n",
       "        [1.09861229, 0.        , 0.        ],\n",
       "        [1.09861229, 0.        , 0.        ],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.81093022, 0.81093022],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511],\n",
       "        [0.        , 0.40546511, 0.40546511]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_articles()\n",
    "init_matrix, ax_Y, ax_X = build_term_by_document_matrix()\n",
    "idf_matrix = tf_to_idf(init_matrix)\n",
    "idf_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Program pozwalający na wprowadzenie zapytania, który następnie przekształci je do reprezentacji wektorowej $q$\n",
    "\n",
    "#### Program ma zwrócić $k$ dokumentów najbardziej zbliżonych do podanego zapytania $q$.\n",
    "\n",
    "Należy użyć korelacji między wektorami jako miary podobieństwa:\n",
    "\n",
    "$cos(\\theta_j) = \\frac{q^T d_j}{||q||\\cdot||d_j||} = \\frac{q^T Ae_j}{||q||\\cdot||Ae_j||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents_beta(query, term_by_document_matrix, terms, documents, k = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * IDF(term_by_document_matrix[i,:])\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for j in range(M):\n",
    "        d_j = term_by_document_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        similarities.append(((q_T * d_j)[0,0] / (q_norm * d_j_norm), documents[j]))\n",
    "\n",
    "    return sorted(similarities, key = lambda x: x[0], reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Po uruchomieniu poniższej komórki, pojawi się pod nią pole tekstowe. Należy w nie wpisać zapytanie q (sekwencja słów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "query = input(\"Podaj zapytanie: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "similar_documents = find_simillar_documents_beta(query, idf_matrix, ax_Y, ax_X)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Zastosowanie normalizacji wektorów $d_j$ i wektora $q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents(query, term_by_document_matrix, terms, documents, k = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * IDF(term_by_document_matrix[i,:])\n",
    "\n",
    "    # normalize d_j\n",
    "    for i in range(M):\n",
    "        norm = sparse.linalg.norm(term_by_document_matrix[:,i])\n",
    "        term_by_document_matrix[:,i] /= norm\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    # normalize q\n",
    "    q_T /= q_norm\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = q_T * term_by_document_matrix\n",
    "    result = []\n",
    "    \n",
    "    for i in range(M):\n",
    "        result.append((similarities[0,i], documents[i]))\n",
    "\n",
    "    return sorted(result, key = lambda x: x[0], reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "similar_documents = find_simillar_documents(query, idf_matrix, ax_Y, ax_X)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. W celu usunięcia szumu z macierzy A zastosujemy SVD i *low rank approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents(query, term_by_document_matrix, terms, documents, limit = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * IDF(term_by_document_matrix[i,:])\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for j in range(M):\n",
    "        d_j = term_by_document_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        similarities.append(((q_T * d_j)[0,0] / (q_norm * d_j_norm), documents[j]))\n",
    "    \n",
    "    for i in range(M):\n",
    "        result.append((similarities[0,i], documents[i]))\n",
    "\n",
    "    return sorted(similarities, key = lambda x: x[0], reverse = True)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
