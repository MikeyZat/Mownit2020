{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Labolatorium 4\n",
    "## Wyszukiwarka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Przygotowanie zbioru (> 1000 elementów) dokumentów tekstowych w języku angielskim.\n",
    "\n",
    "Dokumenty znajdują się w folderze `articles`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. & 3. Określenie słów kluczowych (termów) potrzebnych do wyznaczenia *bag-of-words* dla każdego dokumentów oraz wyznaczenie *bag-of-words* dla każdego dokumentu $d_j$\n",
    "\n",
    "W naszym przypadku bazą termów będzie unia wszystkich słów występujących we wszystkich tekstach. Postaramy się pominąć wszelkiego rodzaju znaki interpunkcyjne oraz często występujące słowa w języku angielskim, które nie mają większego znaczenia (np. 'a', 'this', 'of')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przydatne importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zapisywanie i odczyt najważniejszych struktur z pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def save_counter_to_file(file_name, bag_of_words):\n",
    "    text = \"\"\n",
    "    for word, count in bag_of_words.items():\n",
    "        text = text + word + \" \" + str(count) + \"\\n\"\n",
    "    filepath = os.path.join(os.getcwd(), file_name)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as article_file:\n",
    "        article_file.write(text)\n",
    "\n",
    "        \n",
    "def load_counter_from_file(file_name):\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "    bag_of_words = Counter()\n",
    "\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as text:\n",
    "            for line in text:\n",
    "                elems = line.split()  # elems[0] - word, elems[1] - word count\n",
    "                bag_of_words[elems[0]] = int(elems[1])\n",
    "                \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja, która na podstawie tekstu tworzy wektor *bag-of-words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words_from_text(text):\n",
    "    # make all article lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # divide into words (it still includes punctuation)\n",
    "    words = [word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "\n",
    "    # remove meaningless English words such as 'the', 'a' etc.\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in english_stop_words]\n",
    "\n",
    "    # remove punctuation\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.add(\"...\")\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # stem words - this makes words with the same meaning equal, e.g. responsibility, responsible => respons\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # remove meaningless 1 or 2 chars words\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "\n",
    "    # create a bag_of_words based on this article\n",
    "    bag_of_words = Counter(words)\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funkcja, która dla każdego artykułu:\n",
    "- wczytuje jego tekst i na jego podstawie tworzy wektor cech *bag-of-words* ${d_j}$\n",
    "- Zapisuje ten wektor do odpowiedniego pliku w katalogu `bags`\n",
    "\n",
    "W końcu na podstawie wszystkich *bag-of-words* tworzy słownik bazowy dla wszystkich dokumentów i zapisuje go do pliku `base_dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def process_articles(source_directory = \"articles\"):\n",
    "    base_dictionary = Counter()\n",
    "    \n",
    "    curr_path = os.getcwd()\n",
    "    source_path = os.path.join(curr_path, source_directory)\n",
    "    bags_dir = os.path.join(curr_path, \"bags\")\n",
    "    if not os.path.exists(bags_dir):\n",
    "        os.makedirs(bags_dir)\n",
    "        \n",
    "    for file_name in os.listdir(source_path):\n",
    "        file_path = os.path.join(source_path, file_name)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as text:\n",
    "\n",
    "            bag_of_words = get_bag_of_words_from_text(text.read())\n",
    "            base_dictionary += bag_of_words\n",
    "            save_counter_to_file(f\"bags/{file_name}\", bag_of_words)\n",
    "    \n",
    "    save_counter_to_file(\"base_dictionary\", base_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Budowanie rzadkiej macierzy wektorów cech *term-by-document matrix*.\n",
    "Wektory ułożone są kolumnowo $A_{m \\times n} = [d_1|d_2|...|d_n]$ (m jest liczbą termów w słowniku, n jest liczbą dokumentów)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa funkcja tworzy macierz cech (pierwsza zwracana wartość), ponadtdo zwraca opis osi Y (termy w odpowiedniej kolejności) jako drugą wartość oraz opis osi X (nazwy dokumentów w odpowiedniej kolejności) jako trzecią wartość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_term_by_document_matrix(base_ditionary_name = \"base_dictionary\", bags_dir = \"bags\"):\n",
    "    base_dictionary = load_counter_from_file(base_ditionary_name)\n",
    "    terms_list = list(base_dictionary)\n",
    "    \n",
    "    \n",
    "    bag_names = os.listdir(bags_dir)\n",
    "    N = len(bag_names)\n",
    "    M = len(terms_list)\n",
    "    \n",
    "    term_by_document_matrix = sparse.lil_matrix((M, N))\n",
    "\n",
    "    for j, file_name in enumerate(bag_names):\n",
    "        bag_of_terms = load_counter_from_file(os.path.join(bags_dir, file_name))\n",
    "\n",
    "        \n",
    "        for i, term in enumerate(terms_list):\n",
    "            term_by_document_matrix[i, j] = bag_of_terms[term]\n",
    "            \n",
    "    \n",
    "    return term_by_document_matrix.tocsr(), terms_list, bag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Przetworzenie wstępnie otrzymanego zbioru danych przez *inverse document frequency*. \n",
    "\n",
    "$ IDF(w) = log(\\frac{N}{n_w}) $\n",
    "\n",
    "gdzie:\\\n",
    "$n_w$ - liczba dokumentów, w których występuje słowo $w$\\\n",
    "$N$ - całkowita liczba dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def IDF(row):\n",
    "    N = row.shape[1]\n",
    "    n_w = row.count_nonzero()\n",
    "    return np.log(N / n_w)\n",
    "\n",
    "def tf_to_idf(term_by_document_matrix):\n",
    "    N = term_by_document_matrix.shape[0]\n",
    "    idfs = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        idfs[i] = IDF(term_by_document_matrix[i,:])\n",
    "        term_by_document_matrix[i,:] *= idfs[i]\n",
    "    return term_by_document_matrix, idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Czas przetestować wyżej napisane funkcj i utworzyć pełnoprawną macierz *term-by-document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[180.03779932,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [ 78.78699934,   0.        ,   0.        , ...,   8.75411104,\n",
       "           0.        ,   0.        ],\n",
       "        [  4.46303042,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        ...,\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           7.23561914,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           7.23561914,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          14.47123828,   0.        ]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_articles()\n",
    "init_matrix, ax_Y, ax_X = build_term_by_document_matrix()\n",
    "idf_matrix, idfs = tf_to_idf(init_matrix)\n",
    "idf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Program pozwalający na wprowadzenie zapytania, który następnie przekształci je do reprezentacji wektorowej $q$\n",
    "\n",
    "#### Program ma zwrócić $k$ dokumentów najbardziej zbliżonych do podanego zapytania $q$.\n",
    "\n",
    "Należy użyć korelacji między wektorami jako miary podobieństwa:\n",
    "\n",
    "$cos(\\theta_j) = \\frac{q^T d_j}{||q||\\cdot||d_j||} = \\frac{q^T Ae_j}{||q||\\cdot||Ae_j||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents_beta(query, term_by_document_matrix, terms, documents, k = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * idfs[i]\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for j in range(M):\n",
    "        d_j = term_by_document_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        similarities.append(((q_T * d_j)[0,0] / (q_norm * d_j_norm), documents[j]))\n",
    "\n",
    "    return sorted(similarities, key = lambda x: x[0], reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Po uruchomieniu poniższej komórki, pojawi się pod nią pole tekstowe. Należy w nie wpisać zapytanie q (sekwencja słów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podaj zapytanie: Black people in the japan\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Podaj zapytanie: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page2.txt\n",
      "page0.txt\n",
      "page6.txt\n",
      "page8.txt\n",
      "page915.txt\n",
      "page354.txt\n",
      "page3.txt\n",
      "page881.txt\n",
      "page141.txt\n",
      "page337.txt\n",
      "page1339.txt\n",
      "page338.txt\n",
      "page340.txt\n",
      "page1022.txt\n",
      "page375.txt\n",
      "page36.txt\n",
      "page1125.txt\n",
      "page5.txt\n",
      "page35.txt\n",
      "page355.txt\n"
     ]
    }
   ],
   "source": [
    "similar_documents = find_simillar_documents_beta(query, idf_matrix, ax_Y, ax_X)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Zastosowanie normalizacji wektorów $d_j$ i wektora $q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents(query, term_by_document_matrix, terms, documents, k = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * idfs[i]\n",
    "\n",
    "    # normalize d_j\n",
    "    for i in range(M):\n",
    "        norm = sparse.linalg.norm(term_by_document_matrix[:,i])\n",
    "        term_by_document_matrix[:,i] /= norm\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    # normalize q\n",
    "    q_T /= q_norm\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = q_T * term_by_document_matrix\n",
    "    result = [(similarities[0,i], documents[i]) for i in range(M)]\n",
    "\n",
    "    return sorted(result, key = lambda x: x[0], reverse = True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page2.txt\n",
      "page0.txt\n",
      "page6.txt\n",
      "page8.txt\n",
      "page915.txt\n",
      "page354.txt\n",
      "page3.txt\n",
      "page881.txt\n",
      "page141.txt\n",
      "page337.txt\n",
      "page1339.txt\n",
      "page338.txt\n",
      "page340.txt\n",
      "page1022.txt\n",
      "page375.txt\n",
      "page36.txt\n",
      "page1125.txt\n",
      "page5.txt\n",
      "page35.txt\n",
      "page355.txt\n"
     ]
    }
   ],
   "source": [
    "similar_documents = find_simillar_documents(query, idf_matrix, ax_Y, ax_X)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. W celu usunięcia szumu z macierzy A zastosujemy SVD i *low rank approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_simillar_documents_svd(query, term_by_document_matrix, terms, documents, k, limit = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "    N = len(terms)\n",
    "    M = len(documents)\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * idfs[i]\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    U, S, V_t = sparse.linalg.svds(idf_matrix, k)\n",
    "\n",
    "    Uk = sparse.lil_matrix(U[:, :k])\n",
    "    Sk = sparse.lil_matrix(np.diag(S[:k]))\n",
    "    Vk = sparse.lil_matrix(V_t[:k, :])\n",
    "    filtered_matrix = sparse.csc_matrix(Uk * Sk * Vk)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for j in range(M):\n",
    "        d_j = filtered_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        similarities.append(((q_T * d_j)[0,0] / (q_norm * d_j_norm), documents[j]))\n",
    "\n",
    "    return sorted(similarities, key = lambda x: x[0], reverse = True)[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sprawdzenie wyników wyszukiwania z redukcją szumu dla k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page6.txt\n",
      "page172.txt\n",
      "page32.txt\n",
      "page2.txt\n",
      "page170.txt\n",
      "page39.txt\n",
      "page33.txt\n",
      "page0.txt\n",
      "page1033.txt\n",
      "page30.txt\n",
      "page8.txt\n",
      "page125.txt\n",
      "page130.txt\n",
      "page56.txt\n",
      "page31.txt\n",
      "page52.txt\n",
      "page1376.txt\n",
      "page180.txt\n",
      "page29.txt\n",
      "page341.txt\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "similar_documents = find_simillar_documents_svd(query, idf_matrix, ax_Y, ax_X, k)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9. Porównamy teraz wyniki dla różnych wartości k oraz wyszukiwania bez redukcji szumu. Spróbujemy znaleźć k, dla którego wyniki wyszukiwania są najlepsze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page2.txt\n",
      "page6.txt\n",
      "page8.txt\n",
      "page0.txt\n",
      "page3.txt\n",
      "page5.txt\n",
      "page755.txt\n",
      "page354.txt\n",
      "page7.txt\n",
      "page1022.txt\n",
      "page4.txt\n",
      "page744.txt\n",
      "page489.txt\n",
      "page358.txt\n",
      "page1042.txt\n",
      "page35.txt\n",
      "page341.txt\n",
      "page66.txt\n",
      "page533.txt\n",
      "page375.txt\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "\n",
    "similar_documents = find_simillar_documents_svd(query, idf_matrix, ax_Y, ax_X, k)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page6.txt\n",
      "page2.txt\n",
      "page0.txt\n",
      "page8.txt\n",
      "page3.txt\n",
      "page755.txt\n",
      "page5.txt\n",
      "page1022.txt\n",
      "page358.txt\n",
      "page354.txt\n",
      "page35.txt\n",
      "page7.txt\n",
      "page744.txt\n",
      "page915.txt\n",
      "page489.txt\n",
      "page983.txt\n",
      "page338.txt\n",
      "page340.txt\n",
      "page533.txt\n",
      "page344.txt\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "\n",
    "similar_documents = find_simillar_documents_svd(query, idf_matrix, ax_Y, ax_X, k)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page2.txt\n",
      "page6.txt\n",
      "page0.txt\n",
      "page8.txt\n",
      "page3.txt\n",
      "page354.txt\n",
      "page915.txt\n",
      "page1022.txt\n",
      "page35.txt\n",
      "page755.txt\n",
      "page358.txt\n",
      "page340.txt\n",
      "page881.txt\n",
      "page338.txt\n",
      "page337.txt\n",
      "page375.txt\n",
      "page1134.txt\n",
      "page534.txt\n",
      "page5.txt\n",
      "page36.txt\n"
     ]
    }
   ],
   "source": [
    "k = 400\n",
    "\n",
    "similar_documents = find_simillar_documents_svd(query, idf_matrix, ax_Y, ax_X, k)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subiektywnie oceniam, że dla k=200 otrzymałem najlepsze wartości. Tekst page6.txt znajdujący się na 1 miejscu w wynikach jest rzeczywiście tekstem, którego szukałem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Podsumowanie\n",
    "\n",
    "Stwórzmy zatem gotowy silnik wyszukiwarki, który zwróci nam oczekiwane wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200\n",
    "terms = ax_Y\n",
    "documents = ax_X\n",
    "U, S, V_t = sparse.linalg.svds(idf_matrix, k)\n",
    "\n",
    "Uk = sparse.lil_matrix(U[:, :k])\n",
    "Sk = sparse.lil_matrix(np.diag(S[:k]))\n",
    "Vk = sparse.lil_matrix(V_t[:k, :])\n",
    "filtered_matrix = sparse.csc_matrix(Uk * Sk * Vk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(terms)\n",
    "M = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_simillar_documents_final(query, limit = 20):\n",
    "    q_bag = get_bag_of_words_from_text(query)\n",
    "\n",
    "    q_vector = sparse.lil_matrix((N, 1))\n",
    "    for i in range(N):\n",
    "        q_vector[i, 0] = q_bag[terms[i]] * idfs[i]\n",
    "    \n",
    "    q_norm = sparse.linalg.norm(q_vector)\n",
    "    q_T = q_vector.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for j in range(M):\n",
    "        d_j = filtered_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        similarities.append(((q_T * d_j)[0,0] / (q_norm * d_j_norm), documents[j]))\n",
    "\n",
    "    return sorted(similarities, key = lambda x: x[0], reverse = True)[:limit]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podaj zapytanie: Black people in japan\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Podaj zapytanie: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page6.txt\n",
      "page2.txt\n",
      "page0.txt\n",
      "page8.txt\n",
      "page3.txt\n",
      "page755.txt\n",
      "page5.txt\n",
      "page1022.txt\n",
      "page358.txt\n",
      "page354.txt\n",
      "page35.txt\n",
      "page7.txt\n",
      "page744.txt\n",
      "page915.txt\n",
      "page489.txt\n",
      "page983.txt\n",
      "page338.txt\n",
      "page340.txt\n",
      "page533.txt\n",
      "page344.txt\n"
     ]
    }
   ],
   "source": [
    "similar_documents = find_simillar_documents_final(query)\n",
    "\n",
    "for _, document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
